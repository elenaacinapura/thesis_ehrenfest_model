\chapter{The Formalism of Markov Chains}
\label{ch:markov_chains}
\section{Definitions}
Consider a finite set $\Space$ with dimension $M$ and suppose we have a system whose state can take values in $\Space$; $\Space$ will therefore be called \emph{state space}. Let us indicate the state of the system at time $t$ with the variable $X_t$, and let us furthermore discretize the time, taking $t \in \mathbb{N}$. Suppose now that system state changes randomly in time, so that $X_t$ is a random variable. Let
\begin{equation}
    \prob{X_t = x} \quad , \quad x \in \Space
\end{equation}
be the probability that the system is found the state labeled by $x$ at time $t$.
\begin{ndef} [Discrete Markov chain] We say that a sequence of states $\{X_0, X_1, X_2, \dots, X_t\}$ is a discrete Markov chain if the following property holds $\forall t \in \mathbb N$ and $\forall x_0, \dots, x_t \in \Space$:
    \begin{equation}
        \prob{ X_t = x_t | X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2}, \dots, X_0 = x_0 } = \prob { X_t = x_t | X_{t-1} = x_{t-1} } \quad 
    \end{equation}
\end{ndef}
The above stated property, usually called \emph{Markov property} but also, more evocatively, \emph{memorylessness}, basically states that the probability for the system to be in a certain state at a give time only depends on the state of the time before. This means that the system "has no memory of the past": only the current state can affect the future, and not the way the system has reached it.

\smallskip
We will now consider a particular kind of Markov chains, namely \emph{time-homogeneous} ones.
\begin{ndef} [Time-homogeneous Markov chain] A Markov chain is called time-homogeneous if the transition probability to some certain state to another doesn't depend on time, i.e. if
    \begin{equation}
        \prob {X_{t+1} = y | X_{t} = x} = \prob {X_{t} = y | X_{t-1} = x} \quad \quad \forall t\in \mathbb{N}, \forall y, x \in \Space
    \end{equation}
\end{ndef}

As a consequence of time-homogeneity, we are allowed to talk in general terms about \emph{the} probability of the transition from state $x$ to state $y$, since this quantity does not depend on time and is therefore well defined. For the remainder of this work we will only be dealing with time-homogeneous Markov chains.

\section{Distributions, stochastic matrices and evolution}
Having defined the basic properties of a Markov chain, we will now introduce a powerful means of representing the state and the evolution of a system in a Markov process, which is the use of distribution vectors and stochastic matrices. Since the number of the possible states if finite, the idea is to represent in a vector the probabilities for the system to be in the possible states at a certain time, and in a matrix the probabilities of all the possible transitions between states. Let us now formalize these concepts.

\smallskip
Consider a state space $\Space$ with dimention $M$, and label the possible states with the symbols $x_1, x_2, \dots, x_M$.

\begin{ndef}[Distribution vector]
    A distribution vector, also simply called \emph{distribution}, is a vector $\vec{f} \in \mathbb{R}^M$ with the following properties:
    \begin{center}
        \begin{enumerate}
            \item $f_i \geq 0 \quad \forall i = 1,\dots, M$
            \item $\sum_{i=0}^M f_i = 1$
            \item $f_i = \prob {X = x_i}$
        \end{enumerate}
    \end{center}
    Notice that this is a good definition, since the sum of the probabilities of all the possible states is 1, as it should be, and these probabilities are all positive numbers.
\end{ndef}

Distribution vectors contain all the necessary information to completely describe the state of a system. If, for instance, we know for certain that our system is found in the state $x_k$ at a certain time, then all the elements of its distribution will be 0, except for the k-th one, which will be 1. 
Furthermore, since we will be dealing with the time evolution of the system, we will add a subscript to the distributions to indicate the time to which the distribution refers: $\vec{f}_t$, for instance, will be the distribution of the system at time $t$.

\smallskip
Having seen how to represent the probability distribution of a state, let us now concentrate on how to represent transition probabilities.

\begin{ndef} [Stochastic matrix] \label{def:stoc_matrix}
    A stochastic (or transition) matrix is a matrix $P \in \mathbb{R}^{M\times M}$ with the following properties:
    \begin{center}
        \begin{enumerate}
            \item $P_{ij} \geq 0 \quad \forall i,j = 1,\dots,M$
            \item $\sum_{i = 0}^M P_{ij} = 1 \quad \forall j = 0,\dots,M$ 
            \item $P_{ij} = \prob {X_{t+1} = x_i | X_t = x_j } \quad \forall i,j = 1,\dots,M; \forall t\in \mathbb N$
        \end{enumerate}
    \end{center}
\end{ndef}

Note that while distributions depend on time, stochastic matrices do not, since we are considering time-homogeneous Markov chains, and therefore the transition probabilities are time-independent.

\smallskip
Suppose now that a system is described at a certain time by a distribution $\vec{f}_t$. How can we obtain the distribution of the system in the following times? The answer is to use the theory of conditional probabilities.

\begin{theorem} \label{th:evolution_simple}
    Let $\f_t$ be the distribution of the system at time $t$. Then, the distribution at time $t + 1$ given by
    \begin{equation}
        \f_{t+1} = P \f_t
    \end{equation}
\end{theorem}
\begin{proof}
    For all $i = 1,\dots,M$ we have
    \begin{align}
        (\f_t)_i 
            & = \prob {X_{t+1} = x_i } && \text{for the def. of $\f$}\\
            & = \sum_{j = 0}^M \prob{X_{t+1} = x_i | X_t = x_j} \prob{X_t = x_j} && \text{for the rule of conditional probabilities}\\
            & = \sum_{j = 0}^M P_{ij} (\f_t)_j && \text{for the def. of $P$ and $\f$} \label{eq:matmul1}\\
            & = (P \f_t)_i && \text{for the def. of matrix multiplication} \label{eq:matmul2}
    \end{align}

\end{proof}

\begin{corollary} \label{th:evolution_complete}
    Let $\f_0$ be the initial distribution of the system, at time 0. Then, the distribution at time $t$ given by
    \begin{equation}
        \f_t = P^t \f_0
    \end{equation}
\end{corollary}

\begin{remark}
    It is clear from the proof of theorem \ref{th:evolution_simple}, in particular from eq. \ref{eq:matmul1}, that it is useful to interpret distributions as \emph{column vectors}. This, indeed, follows from the fact that we have defined $P$ such that the sum on its \emph{columns} must always equal 1. However, it is also a common practice to define $P$ such that the sum on it \emph{rows} must equal 1; after this latter convention, then, distributions are interpreted as row vectors and one obtains that the law fot the evolution is $\f_{t+1} = \f_t P$. We decided to adhere to the "column-based" convention because it resembles the formalism of quantum mechanics, where operators are multiplied by the state vectors, and not the other way around; this convention results therefore more natural from a physicist's point of view.
\end{remark}

\section{Limiting behaviour}
We now draw our attention to a fundamental aspect of Markov chains. Suppose that the initial distribution of a system, $\f_0$, is completely known, i.e. that we know the probabilities for the system to be in each of the possible states $x_1, x_2, \dots, x_M$. We know how to compute che evolution of these probabilities time step after time step using the stochastic matrix, but we might want to ask ourself the following question: 
\begin{quote}
    \emph{Is there a distribution that the system will eventually reach, asymptotically? If it does, which distribution is this?}
\end{quote}

It is clear why one should be interested in this question: in most real-life applications, one usually starts with a system in a specific known state and lets it evolve for some amount of time, which is usually many orders of magnitude greater than the "typical" time scale of the system. It is therefore useful to understand if the system has an asymptotic behavior.

\smallskip
In order to answer this question, we will introduce some concepts known as \emph{limiting distribution} and \emph{stationary distribution}.

\begin{ndef}[Limiting distribution]
Given a Markov chain with stochastic matrix $P$, its limiting distribution $\p$, if it does exist, is defined as the distribution whose components are
\begin{equation}
    \pi_i \equiv \lim_{t \rightarrow \infty} \prob{X_t = x_i} \quad \forall i=1,\dots,M
\end{equation}
independently of the initial distribution of the chain.
\end{ndef}

A nice feature of the limiting distribution is that, if it exists, then the matrix obtained by $\lim_{t \rightarrow \infty} P^t$ has each column equal to the limiting distribution:
\begin{equation}
    \lim_{t \rightarrow \infty} P^t = 
    \begin{pmatrix}
        \pi_1 & \pi_1 & \dots & \pi_1 \\
        \pi_2 & \pi_2 & \dots & \pi_2 \\
        \vdots & \vdots & \vdots & \vdots \\
        \pi_M & \pi_M & \dots & \pi_M
    \end{pmatrix}
\end{equation}
This is quite straightforward indeed, thanks to corollary \ref{th:evolution_complete}:
\begin{align}
    \pi_i
        & = \lim_{t \rightarrow \infty} \prob{X_t = x_i} \\
        & = \lim_{t \rightarrow \infty} (\f_t)_i \\
        & = \lim_{t \rightarrow \infty} P^t \f_0 && \text{for corollary \ref{th:evolution_complete}}
\end{align}
Since, by definition, the $\pi_i$ must not depend on $\f_0$, then we can choose as $\f_0$ the vectors of the canonical basis of $\mathbb{R}^M$; each $i$-th element of the canonical basis is then transformed in the $i$-th column of $P^t$, and thus we obtain the above equality. 

\medskip
As mentioned before, another important concept that we have to introduce in the context of limiting behaviour is the \emph{stationary distribution}.

\begin{ndef} [Stationary distribution]
    Given a Markov chain with stochastic matrix $P$, its stationary distribution $\staz$, if it does exist, is defined as the distribution such that 
    \begin{equation}
        P \staz = \staz
    \end{equation}
    Hence, the stationary distribution is the \text{eigenvalue} of the stochastic matrix with eigenvalue equal 1. If the system happens to be found in the stationary distribution, it will remain in that same state forever.
\end{ndef}
As one might guess, there is a strong relationship between the limiting distribution and the stationary distribution, and that is expressed by the following theorem:

\begin{theorem}
    Suppose that a Markov chain with stochastic matrix $P$ has a limiting distribution $\p$. Then $\p$ is also a stationary distribution for the chain.
\end{theorem}
\begin{proof}
    We have that, independently of the initial distribution $\f_0$
    \begin{align}
        P \p
            & = P \lim_{t \rightarrow \infty} P^t \f_0 \\
            & = lim_{t \rightarrow \infty} P^{t+1} \f_0 \\
            & = \p
    \end{align}
\end{proof}
Unfortunately, the other implication does not always hold. That is, a Markov chain might well have a stationary distribution, but $\lim_{t \rightarrow \infty} P^t$ may not exist anyway. First of all, one must assure that such stationary distribution is unique, otherwise the limit would not be unique; even then, however, other assumptions are needed in order to guarantee convergence. Fortunately, the theorem named after Perron and Frobenius states when the stationary distribution of a chain is also its limiting distribution, but before we proceed with its statement we must give just some more definitions. We will not prove the theorem, since that would be a lengthy task that would draw our attention away from the physical applications of the Markov chains, so we will also try to give an intuitive and practical definition of the following properties, rather than a strictly formal one. 
% Agggiungere reference a dove trovare i dettagli dei calcoli

\begin{ndef} [Irreducible Markov chain]
    A Markov chain is said to be irreducible if every state can lead, sooner or later, to every other state. That is, no matter what the initial state for the system is at $t=0$, every other state has a nonzero probability to be reach at some time $t>0$.
\end{ndef}
\begin{ndef} [Period of a state and aperiodicity]
    Given a state $x_i \in \Space$, its \emph{period} $k_i$ is defined as the greatest common divisor (gcd) of the times at which $x_i$ can be reached, given that the system started in $x_i$ at $t=0$. That is,
    \begin{equation}
        k_i \equiv \text{gcd} \left\{ t>0 : \prob{X_t = x_i | X_0 = x_i} > 0 \right\}
    \end{equation} 
    A \emph{state} is said to be \emph{aperiodic} if its period equals 1. A Markov chain on space state $\Space$ is said to be aperiodic if all $x_i \in \Space$ are aperiodic.
\end{ndef}
Now we are finally equipped to state the Perron-Frobenius theorem.

\begin{theorem}[Perron-Frobenius]
    Suppose that a Markov chain is irreducible and aperiodic. Then it has a unique stationary distribution and a limiting distribution, and the two coincide.
\end{theorem}
As a side note, a Markov chain which is irreducible and aperiodic is usually called \emph{ergodic}.
So we have finally found an answer to our original question: if we can verify that a Markov chain is ergodic, then all we have to do is to find its stationary distribution (i.e., the eigenvector with unitary eigenvalue, and we are usually good at finding eigenvalues), and then we have automatically found the asymptotic distribution of the system! In the following paragraph, then, we will show which interesting information about the Markov chain we can find, thanks to the knowledge of the limiting distribution.

\section{Mean recurrence time and the stationary distribution}
